#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import h5py
import numpy as np
import pandas as pd
from keras.models import Model
from keras.layers import Input,Activation,Dense,Flatten, Dropout, Embedding, Maximum
from keras.layers import Conv1D, MaxPooling1D,AveragePooling1D, ZeroPadding1D, GlobalAveragePooling1D, CuDNNLSTM
from keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.model_selection import StratifiedKFold

def conv_lstm():
    embed_size = 301
    num_filters = 64
    maxlen = 7000
    kernel_size = [3, 5, 7]
    main_input = Input(shape=(maxlen,))
    emb = Embedding(embed_size, 256, input_length=maxlen)(main_input)
    # _embed = SpatialDropout1D(0.15)(emb)
    warppers = []
    warppers2 = [] 
    warppers3 = []
    for _kernel_size in kernel_size:
        conv1d = Conv1D(filters=num_filters, kernel_size=_kernel_size, activation='relu', padding='same')(emb)
        warppers.append(AveragePooling1D(2)(conv1d))
    for (_kernel_size, cnn) in zip(kernel_size, warppers):
        conv1d_2 = Conv1D(filters=num_filters, kernel_size=_kernel_size, activation='relu', padding='same')(cnn)
        warppers2.append(AveragePooling1D(2)(conv1d_2))
    for (_kernel_size, cnn) in zip(kernel_size, warppers2):
        conv1d_2 = Conv1D(filters=num_filters, kernel_size=_kernel_size, activation='relu', padding='same')(cnn)
        warppers3.append(AveragePooling1D(2)(conv1d_2))
    fc = Maximum()(warppers3)
    rl = CuDNNLSTM(128)(fc)
    main_output = Dense(8, activation='softmax')(rl)
    model = Model(inputs=main_input, outputs=main_output)
    return model


def main():
    # load feature data
    file = h5py.File("./data/malware_seq_data_7000.h5", "r+")
    print(list(file.keys()))
    pad_seq_train = np.array(file["pad_seq_train"])
    pad_seq_test = np.array(file["pad_seq_test"])
    train_label = np.array(file["train_label"])

    # class weight
    cls_wt = pd.Series(train_label).value_counts()
    cls_wt = cls_wt.sum()/cls_wt

    # skf5 model fit
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    result_val = np.zeros(shape=(len(pad_seq_train), 8))
    result_test = np.zeros(shape=(len(pad_seq_test), 8))
    for i, (tr_index, val_index) in enumerate(skf.split(pad_seq_train, train_label)):
        tr_X, val_X = pad_seq_train[[tr_index]], pad_seq_train[[val_index]]
        tr_y, val_y = train_label[[tr_index]], train_label[[val_index]]
        tr_y = pd.get_dummies(tr_y).values
        val_y = pd.get_dummies(val_y).values
        model = conv_lstm()
        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
        model_weight_name = "./data/weights/Model_conv_lstm_7k_weight_%s.pkl" % i
        early_stopping = EarlyStopping(monitor='val_loss', patience=10)
        checkpoint = ModelCheckpoint(model_weight_name, save_best_only=True, save_weights_only=True)
        # tensorBoard = keras.callbacks.TensorBoard(log_dir="./data/TXTCNN/Graph", histogram_freq=1, write_graph=True, write_images=True)

        model.fit(tr_X, tr_y,
                  validation_data=(val_X, val_y),
                  epochs=100, batch_size=32,
                  shuffle=True,
                  class_weight=cls_wt,
                  callbacks=[early_stopping, checkpoint])
        model.load_weights(model_weight_name)
        pred_val_X = model.predict(val_X, batch_size=64, verbose=1)
        pred_test = model.predict(pad_seq_test, batch_size=64, verbose=1)
        result_val[[val_index]] = pred_val_X
        result_test += pred_test
    result_test /= 5


    # save result
    pd.DataFrame(result_val).to_csv("./data/results/cnn_lstm_train_5f.csv")
    pd.DataFrame(result_test).to_csv("./data/results/cnn_lstm_test_5average.csv")


if __name__ == "__main__":
    main()
